# Web Codegen Scorer

This project is a tool designed to assess the quality of front-end code generated by Large Language Models (LLMs).

## Documentation directory

- [Environment config reference](./docs/environment-reference.md)
- [How to set up a new model?](./docs/model-setup.md)

## Setup

1.  **Install the package:**
```bash
npm install -g web-codegen-scorer
```

2.  **Set up your API keys:**
In order to run an eval, you have to specify an API keys for the relevant providers as environment variables:
```bash
export GEMINI_API_KEY="YOUR_API_KEY_HERE" # If you're using Gemini models
export OPENAI_API_KEY="YOUR_API_KEY_HERE" # If you're using OpenAI models
export ANTHROPIC_API_KEY="YOUR_API_KEY_HERE" # If you're using Anthropic models
```

3. **Run an eval:**
You can run your first eval using our Angular example with the following command:
```bash
web-codegen-scorer eval --env=angular-example
```

4. (Optional) **Set up your own eval:**
If you want to set up a custom eval, instead of using our built-in examples, you can run the following
command which will guide you through the process:

```bash
web-codegen-scorer init
```

## Command-line flags

You can customize the `web-codegen-scorer eval` script with the following flags:

- `--env=<path>` (alias: `--environment`): (**Required**) Specifies the path from which to load the environment config.
  - Example: `web-codegen-scorer eval --env=foo/bar/my-env.js`

- `--model=<name>`: Specifies the model to use when generating code. Defaults to the value of `DEFAULT_MODEL_NAME`.
  - Example: `web-codegen-scorer eval --model=gemini-2.5-flash --env=<config path>`

- `--runner=<name>`: Specifies the runner to use to execute the eval. Supported runners are `genkit` (default) or `gemini-cli`.

- `--local`: Runs the script in local mode for the initial code generation request. Instead of calling the LLM, it will attempt to read the initial code from a corresponding file in the `.llm-output` directory (e.g., `.llm-output/todo-app.ts`). This is useful for re-running assessments or debugging the build/repair process without incurring LLM costs for the initial generation.
  - **Note:** You typically need to run `web-codegen-scorer eval` once without `--local` to generate the initial files in `.llm-output`.
  - The `web-codegen-scorer eval:local` script is a shortcut for `web-codegen-scorer eval --local`.

- `--limit=<number>`: Specifies the number of application prompts to process. Defaults to `5`.
  - Example: `web-codegen-scorer eval --limit=10 --env=<config path>`

- `--output-directory=<name>` (alias: `--output-dir`): Specifies which directory to output the generated code under which is useful for debugging. By default the code will be generated in a temporary directory.
  - Example: `web-codegen-scorer eval --output-dir=test-output --env=<config path>`

- `--concurrency=<number>`: Sets the maximum number of concurrent AI API requests. Defaults to `5` (as defined by `DEFAULT_CONCURRENCY` in `src/config.ts`).
  - Example: `web-codegen-scorer eval --concurrency=3 --env=<config path>`

- `--report-name=<name>`: Sets the name for the generated report directory. Defaults to a timestamp (e.g., `2023-10-27T10-30-00-000Z`). The name will be sanitized (non-alphanumeric characters replaced with hyphens).
  - Example: `web-codegen-scorer eval --report-name=my-custom-report --env=<config path>`

- `--rag-endpoint=<url>`: Specifies a custom RAG (Retrieval-Augmented Generation) endpoint URL. The URL must contain a `PROMPT` substring, which will be replaced with the user prompt.
  - Example: `web-codegen-scorer eval --rag-endpoint="http://localhost:8080/my-rag-endpoint?query=PROMPT" --env=<config path>`

- `--prompt-filter=<name>`: String used to filter which prompts should be run. By default a random sample (controlled by `--limit`) will be taken from the prompts in the current environment. Setting this can be useful for debugging a specific prompt.
  - Example: `web-codegen-scorer eval --prompt-filter=tic-tac-toe --env=<config path>`

- `--skip-screenshots`: Whether to skip taking screenshots of the generated app. Defaults to `false`.
  - Example: `web-codegen-scorer eval --skip-screenshots --env=<config path>`

- `--labels=<label1> <label2>`: Metadata labels that will be attached to the run.
  - Example: `web-codegen-scorer eval --labels my-label another-label --env=<config path>`

- `--mcp`: Whether to start an MCP for the evaluation. Defaults to `false`.
  - Example: `web-codegen-scorer eval --mcp --env=<config path>`

- `--help`: Prints out usage information about the script.

## Local development

If you've cloned this repo and want to work on the tool, you have to install its dependencies by running `pnpm install`.
Once they're installed, you can run the following commands:

* `pnpm run release-build` - Builds the package in the `dist` directory for publishing to npm.
* `pnpm run eval` - Runs an eval from source.
* `pnpm run report` - Runs the report app from source.
* `pnpm run init` - Runs the init script from source.
* `pnpm run format` - Formats the source code using Prettier.
