# Environment configuration reference

Environments are configured by creating a `config.js` that exposes an object that satisfies the
`EnvironmentConfig` interface. This document covers all the possible options in `EnvironmentConfig`
and what they do.

## Required properties

These properties all have to be specified in order for the environment to function

### `displayName`

Human-readable name that will be shown in eval reports about this environment.

### `id`

Unique ID for the environment. If ommitted, one will be generated from the `displayName`.

### `clientSideFramework`

ID of the client-side framework that the environment will be running, for example `angular`.

### `ratings`

An array defining the ratings that will be executed as a part of the evaluation.
The ratings determine what score that will be assigned to the test run.
Currently we support the following types of ratings:

- `PerBuildRating` - assigns a score based on the build result of the generated code, e.g.
  "Does it build on the first run?" or "Does it build after X repair attempts?"
- `PerFileRating` - assigns a score based on the content of individual files generated by the LLM.
  Can be run either against all file types by setting the `filter` to
  `PerFileRatingContentType.UNKNOWN` or against specific files.
- `LLMBasedRating` - rates the generated code by asking an LLM to assign a score to it,
  e.g. "Does this app match the specified prompts?"

### `packageManager`

Name of the package manager to use to install dependencies for the evaluated code.
Supports `npm`, `pnpm` and `yarn`. Defaults to `npm`.

### `generationSystemPrompt`

Relative path to the system instructions that should be passed to the LLM when generating code.

### `repairSystemPrompt`

Relative path to the system instructions that should be passed to the LLM when repairing failures.

### `executablePrompts`

Configures the prompts that should be evaluated against the environment. Can contain either strings
which represent glob patterns pointing to text files with the prompt's text
(e.g. `./prompts/**/*.md`) or `MultiStepPrompt` objects ([see below](#multi-step-prompts)).
The prompts can be shared between environments
(e.g. `executablePrompts: ['../some-other-env/prompts/**/*.md']`).

### `classifyPrompts`

When enabled, the system prompts for this environment won't be included in the final report.
This is useful when evaluating confidential code.

### `skipInstall`

Whether to skip installing dependencies during the eval run. This can be useful if you've already
ensured that all dependencies are installed through something like pnpm workspaces.

### Prompt templating

Prompts are typically stored in `.md` files. We support the following template syntax inside of
these files in order to augment the prompt and reduce boilerplate:

- `{{> embed file='../path/to/file.md' }}` - embeds the content of the specified file in the
  current one.
- `{{> contextFiles '**/*.foo' }}` - specifies files that should be passed to the LLM as context
  when the prompt is executed. Should be a comma-separated string of glob pattern **within** the
  environments project code. E.g. `{{> contextFiles '**/*.ts, **/*.html' }}` will pass all `.ts`
  and `.html` files as context.
- `{{CLIENT_SIDE_FRAMEWORK_NAME}}` - insert the name of the client-side framework of the current
  environment.
- `{{FULL_STACK_FRAMEWORK_NAME}}` - insert the name of the full-stack framework of the current
  environment.

### Prompt-specific ratings

If you want to run a set of ratings against a specific prompt, you can set an object literal
in the `executablePrompts` array, instead of a string:

```ts
executablePrompts: [
  // Runs only with the environment-level ratings.
  './prompts/foo/*.md',

  // Runs the ratings specific to the `contact-form.md`, as well as the environment-level ones.
  {
    path: './prompts/bar/contact-form.md',
    ratings: contactFormSpecificRatings,
  },
];
```

### Multi-step prompts

Multi-step prompts are prompts meant to evaluate workflows made up of one or more stages.
Steps execute one after another **inside the same directory**, but are rated individually and
snapshots after each step are stored in the final report. You can create a multi-step prompt by
passing an instrance of the `MultiStepPrompt` class into the `executablePrompts` array, for example:

```ts
executablePrompts: [
  new MultiStepPrompt('./prompts/about-page', {
    'step-1': ratingsForFirstStep,
    'step-2': [...ratingsForFirstStep, ratingsForSecondStep],
  }),
];
```

The first parameter is the directory from which to resolve the individual step prompts.
All files in the directory **have to be named `step-{number}.md`**, for example:

**my-env/prompts/about-page/step-1.md:**

```
Create an "About us" page.
```

**my-env/prompts/about-page/step-2.md:**

```
Add a contact form to the "About us" page
```

**my-env/prompts/about-page/step-3.md:**

```
Make it so submitting the contact form redirects the user back to the homepage.
```

The second parameter of `MultiStepPrompt` defines ratings that should be run only against specific
steps. The key is the name of the step (e.g. `step-2`) while the value are the ratings that should
run against it.

## Optional properties

These properties aren't required for the environment to run, but can be used to configure it further.

### `sourceDirectory`

Project into which the LLM-generated files will be placed, built, executed and evaluated.
Can be an entire project or a handful of files that will be merged with the
`projectTemplate` ([see below](#projecttemplate))

### `projectTemplate`

Used for reducing the boilerplate when setting up an environment, `projectTemplate` specifies the
path of the project template that will be merged together with the files from `sourceDirectory` to
create the final project structure that the evaluation will run against.

For example, if the config has `projectTemplate: './templates/angular', sourceDirectory: './project'`,
the eval runner will copy the files from `./templates/angular` into the output directory
and then apply the files from `./project` on top of them, merging directories and replacing
overlapping files.

### `fullStackFramework`

Name of the full-stack framework that is used in the evaluation, in addition to the
`clientSideFramework`. If omitted, the `fullStackFramework` will be set to the same value as
the `clientSideFramework`.

### `mcpServers`

IDs of Model Context Protocol servers that will be started and exposed to the LLM as a part of
the evaluation.

### `buildCommand`

Command used to build the generated code as a part of the evaluation.
Defaults to `<package manager> run build`.

### `serveCommand`

Command used to start a local dev server as a part of the evaluation.
Defaults to `<package manager> run start --port 0`.
